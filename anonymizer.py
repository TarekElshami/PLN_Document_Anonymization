from xml.dom import minidom
import requests
import xml.etree.ElementTree as ET
import os
import tiktoken
import ast
import argparse
from collections import Counter
import logging
from tqdm import tqdm

# Configuraci√≥n de logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('anonymization.log'),
        # logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Mapeo de etiquetas espec√≠ficas a categor√≠as generales
TAG_CATEGORIES = {
    # NAME (Nombre)
    'NOMBRE_SUJETO_ASISTENCIA': 'NAME',
    'NOMBRE_PERSONAL_SANITARIO': 'NAME',

    # PROFESSION (Profesi√≥n)
    'PROFESION': 'PROFESSION',

    # LOCATION (Ubicaci√≥n)
    'HOSPITAL': 'LOCATION',
    'INSTITUCION': 'LOCATION',
    'CALLE': 'LOCATION',
    'TERRITORIO': 'LOCATION',
    'PAIS': 'LOCATION',
    'ID_CENTRO_DE_SALUD': 'LOCATION',

    # AGE (Edad)
    'EDAD_SUJETO_ASISTENCIA': 'AGE',

    # DATE (Fechas)
    'FECHAS': 'DATE',

    # CONTACT (Contacto)
    'NUMERO_TELEFONO': 'CONTACT',
    'NUMERO_FAX': 'CONTACT',
    'CORREO_ELECTRONICO': 'CONTACT',
    'URL_WEB': 'CONTACT',

    # ID (Identificadores)
    'ID_ASEGURAMIENTO': 'ID',
    'ID_CONTACTO_ASISTENCIAL': 'ID',
    'NUMERO_BENEF_PLAN_SALUD': 'ID',
    'IDENTIF_VEHICULOS_NRSERIE_PLACAS': 'ID',
    'IDENTIF_DISPOSITIVOS_NRSERIE': 'ID',
    'IDENTIF_BIOMETRICOS': 'ID',
    'ID_SUJETO_ASISTENCIA': 'ID',
    'ID_TITULACION_PERSONAL_SANITARIO': 'ID',
    'ID_EMPLEO_PERSONAL_SANITARIO': 'ID',
    'OTRO_NUMERO_IDENTIF': 'ID',

    # OTHER (Otros)
    'SEXO_SUJETO_ASISTENCIA': 'OTHER',
    'FAMILIARES_SUJETO_ASISTENCIA': 'OTHER',
    'OTROS_SUJETO_ASISTENCIA': 'OTHER',
    'DIREC_PROT_INTERNET': 'OTHER'
}

PROMPT_BASE = """
Tarea:
Dado un texto cl√≠nico, identifica y anota **todas las entidades de informaci√≥n sensible protegida** (ISP) siguiendo las directrices **estrictas** de la gu√≠a oficial de anotaci√≥n del Plan de Impulso de las Tecnolog√≠as del Lenguaje para informaci√≥n de salud protegida.

üéØ Objetivos:
1. Detectar TODAS las menciones de informaci√≥n sensible en el texto.
2. Etiquetarlas usando esta sintaxis: <<<ETIQUETA>>>texto<<</ETIQUETA>>>
3. Extraer dichas entidades agrupadas por tipo en un JSON v√°lido.

CATEGOR√çAS DE ETIQUETAS (USA SOLO ESTAS, NO INVENTES NINGUNA)
NOMBRE_SUJETO_ASISTENCIA: Solo el nombre y apellidos del paciente. Tambi√©n iniciales, apodos o motes.
NOMBRE_PERSONAL_SANITARIO: Nombre y apellidos de m√©dicos, enfermeros, t√©cnicos u otro personal cl√≠nico.
FAMILIARES_SUJETO_ASISTENCIA: Nombres, apellidos o datos personales de familiares del paciente (edad, parentesco, n√∫mero).
ID_SUJETO_ASISTENCIA: C√≥digos como NHC, CIPA, DNI, NIF, pasaporte u otros identificadores del paciente.
ID_TITULACION_PERSONAL_SANITARIO: N√∫mero de colegiado o licencia del profesional sanitario.
ID_CONTACTO_ASISTENCIAL: Identificador de episodios cl√≠nicos o procesos.
ID_ASEGURAMIENTO: N√∫mero de afiliaci√≥n a la seguridad social (NASS).
EDAD_SUJETO_ASISTENCIA: Edad del paciente (incluyendo formas como "tres d√≠as", "6 a√±os").
SEXO_SUJETO_ASISTENCIA: Sexo del paciente (incluyendo formas como "var√≥n", "ni√±a", "M", "H").
FECHAS: Cualquier fecha del calendario (de nacimiento, ingreso, evoluci√≥n, etc.).
CALLE: Direcci√≥n postal completa, incluyendo tipo de v√≠a, nombre, n√∫mero, piso, etc..
TERRITORIO: Ciudad, provincia, c√≥digo postal, barrio, comarca, o cualquier otra divisi√≥n geogr√°fica.
PAIS: Pa√≠s mencionado en el texto.
CORREO_ELECTRONICO: Cualquier direcci√≥n de correo electr√≥nico.
NUMERO_TELEFONO: N√∫meros de tel√©fono personales o profesionales.
NUMERO_FAX: N√∫meros de fax asociados a la atenci√≥n o el paciente.
DIREC_PROT_INTERNET: Direcciones de protocolo de Internet (IP, TCP, SMTP, etc.).
URL_WEB: Cualquier direcci√≥n web o enlace.
PROFESION: Profesi√≥n del paciente o familiares.
HOSPITAL: Nombres de hospitales o centros sanitarios.
ID_CENTRO_DE_SALUD: Nombres de centros de salud o unidades cl√≠nicas.
INSTITUCION: Cualquier otra instituci√≥n no m√©dica identificable.
NUMERO_IDENTIF: Otros n√∫meros de identificaci√≥n no clasificados.
IDENTIF_VEHICULOS_NRSERIE_PLACAS: Matr√≠culas o n√∫meros de bastidor de veh√≠culos.
IDENTIF_DISPOSITIVOS_NRSERIE: Identificadores de dispositivos m√©dicos (serie, chip, etc.).
IDENTIF_BIOMETRICOS: Huellas, escaneos o cualquier identificador biom√©trico.
OTROS_SUJETO_ASISTENCIA: Cualquier informaci√≥n adicional que pueda permitir la identificaci√≥n del paciente y no est√© incluida en las categor√≠as anteriores.

‚ö†Ô∏è ACLARACIONES
- Cuando haya **varios formatos de una misma entidad** (ej. "3 a√±os" y "tres a√±os"), **an√≥talos todos** por separado, no ignores duplicados sem√°nticos.
- Reconoce **todas las formas de expresar el sexo** del paciente: M, F, var√≥n, mujer, ni√±o, ni√±a, masculino, femenino‚Ä¶

üßæ Reglas de anotaci√≥n (estrictas y actualizadas)
1. No incluir dentro de las etiquetas claves del formulario como "Nombre:", "Edad:", etc..
2. No incluir espacios ni signos de puntuaci√≥n dentro de las etiquetas.
3. Debes incluir todas las apariciones de las entidades aunque se repitan
4. M√∫ltiples palabras que constituyen una sola entidad deben etiquetarse juntas.
5. Dejar fuera de las etiquetas t√≠tulos o prefijos como "Dr.", "D√±a.".
6. Toda informaci√≥n sensible debe ser etiquetada si aparece, sin omitir ninguna.
7. **Si no hay entidades sensibles**, devuelve el JSON con `"entidades": {{}}`.
8. **No generes etiquetas nuevas ni marques datos que no figuren en la lista.**
9. Usa el contexto del documento para distinguir entre entidades del paciente vs. familiares.

‚úÖ Salida esperada (formato obligatorio)
√öNICAMENTE un **JSON v√°lido**, sin explicaciones, sin formato de bloque de c√≥digo ni comentarios. Estructura esperada:
{{
  "texto_anotado": "Texto con etiquetas <<<ETIQUETA>>>...<<</ETIQUETA>>> ya insertadas",
  "entidades": {{
    "ETIQUETA1": ["valor1", "valor2", ...],
    "ETIQUETA2": ["valor1", ...]
  }}
}}
Devu√©lveme solo el JSON como texto plano, sin comillas invertidas, sin markdown ni delimitadores de c√≥digo. No uses etiquetas tipo ```json ni ning√∫n bloque de c√≥digo.
El texto que debes analizar es este:
 "{texto_clinico}"
"""

MODEL_NAME = "llama3.3"
DEFAULT_OLLAMA_PORT = "20201"

# Diccionario de l√≠mites de contexto por modelo
MODEL_CONTEXT_LIMITS = {
    "llama3.2:1B": 2048,
    "llama3.3": 2048
}

SAFETY_MARGIN = 0.1  # 10% de margen de seguridad

USE_GRAMMAR = True

def quitar_tildes(texto):
    reemplazos = {
        '√°': 'a',
        '√©': 'e',
        '√≠': 'i',
        '√≥': 'o',
        '√∫': 'u',
    }
    return ''.join(reemplazos.get(c, c) for c in texto)


def build_meddocan_xml_from_entities(original_text, entidades_json):
    root = ET.Element("MEDDOCAN")
    text_elem = ET.SubElement(root, "TEXT")
    text_elem.text = original_text
    tags_elem = ET.SubElement(root, "TAGS")

    tag_id = 1

    for tag, values in entidades_json.items():
        xml_tag = TAG_CATEGORIES.get(tag)
        if xml_tag is None:
            tag_sin_tildes = quitar_tildes(tag)
            xml_tag = TAG_CATEGORIES.get(tag_sin_tildes, "WARNING")
        value_counts = Counter(values)

        for value, count in value_counts.items():
            start_idx = 0
            occurrences = 0

            while occurrences < count:
                start = original_text.find(value, start_idx)
                if start == -1:
                    break
                end = start + len(value)
                ET.SubElement(tags_elem, xml_tag, {
                    "id": str(tag_id),
                    "start": str(start),
                    "end": str(end),
                    "text": value,
                    "TYPE": tag
                })
                tag_id += 1
                start_idx = end
                occurrences += 1

    return ET.tostring(root, encoding="unicode", method="xml")


def get_gbnf_grammar():
    etiquetas = list(TAG_CATEGORIES.keys())
    etiquetas_union = " | ".join(f'"{tag}"' for tag in etiquetas)

    return f"""
root ::= '{{' '"texto_anotado":' string ',' '"entidades":' '{{' (entidad (',' entidad)*)? '}}' '}}'

entidad ::= etiqueta ':' '[' valores ']'
etiqueta ::= {etiquetas_union}
valores ::= valor (',' valor)*
valor ::= string
string ::= '"' chars '"'
chars ::= char*
char ::= [^"\\n]
""".strip()


def split_text_by_newline(text, max_safe_tokens):
    """
    Divide el texto por saltos de l√≠nea para mantener cada fragmento bajo el l√≠mite de tokens.
    Devuelve una lista de fragmentos seguros.
    """
    encoding = tiktoken.get_encoding("cl100k_base")
    lines = text.split('\n')
    chunks = []
    current_chunk = []

    logger.info("Iniciando divisi√≥n del texto en fragmentos...")
    logger.info(f"L√≠neas totales: {len(lines)}")
    logger.info(f"L√≠mite seguro de tokens por fragmento: {max_safe_tokens}")

    for i, line in enumerate(lines):
        if line.strip() == "":
            continue  # Ignorar l√≠neas vac√≠as

        temp_chunk = '\n'.join(current_chunk + [line])
        prompt_with_chunk = PROMPT_BASE.format(texto_clinico=temp_chunk)
        tokens = encoding.encode(prompt_with_chunk)

        logger.debug(f"L√≠nea {i + 1}: Tokens totales del actual fragmento= {len(tokens)}")

        if len(tokens) < max_safe_tokens:
            current_chunk.append(line)
            logger.debug("A√±adida al fragmento actual")
        else:
            if current_chunk:
                chunks.append('\n'.join(current_chunk))
                logger.info(f"Fragmento completado ({len(chunks)} fragmentos hasta ahora)")
                logger.debug(f"Contenido del fragmento:\n{'-' * 30}\n{chunks[-1]}\n{'-' * 30}")
                current_chunk = [line]
            else:
                # L√≠nea demasiado larga para el prompt incluso sola
                chunks.append(line)
                logger.warning(
                    f"L√≠nea individual demasiado larga, creando fragmento especial ({len(chunks)} fragmentos hasta ahora)")
                logger.debug(f"Contenido del fragmento:\n{'-' * 30}\n{chunks[-1]}\n{'-' * 30}")
                current_chunk = []

    if current_chunk:
        chunks.append('\n'.join(current_chunk))
        logger.info(f"√öltimo fragmento completado ({len(chunks)} fragmentos en total)")
        logger.debug(f"Contenido del fragmento:\n{'-' * 30}\n{chunks[-1]}\n{'-' * 30}")

    logger.info(f"Divisi√≥n completada. Total de fragmentos creados: {len(chunks)}")
    return chunks


def extract_entities(text, ollama_port):
    """
    Procesa el texto con el modelo, dividi√©ndolo si excede el l√≠mite de tokens.
    Devuelve el texto anotado y todas las entidades combinadas.
    """
    encoding = tiktoken.get_encoding("cl100k_base")
    if MODEL_NAME not in MODEL_CONTEXT_LIMITS:
        raise ValueError(f"Modelo '{MODEL_NAME}' no encontrado en MODEL_CONTEXT_LIMITS. "
                         f"Modelos disponibles: {list(MODEL_CONTEXT_LIMITS.keys())}")

    max_tokens = MODEL_CONTEXT_LIMITS[MODEL_NAME]
    safe_max_tokens = int(max_tokens * (1 - SAFETY_MARGIN))

    prompt = PROMPT_BASE.format(texto_clinico=text)
    total_tokens = len(encoding.encode(prompt))

    logger.info("Iniciando extracci√≥n de entidades...")
    logger.info(f"Modelo seleccionado: {MODEL_NAME}")
    logger.info(f"Context limit definido: {max_tokens} tokens")
    logger.info(f"Tokens totales del texto completo: {total_tokens}")
    logger.info(f"L√≠mite seguro de tokens (con margen del {SAFETY_MARGIN * 100}%): {safe_max_tokens}")
    logger.info(f"Puerto Ollama utilizado: {ollama_port}")

    # Determinar si hay que dividir
    if total_tokens > safe_max_tokens:
        logger.warning("Dividiendo el texto por l√≠neas debido al l√≠mite de tokens...")
        text_parts = split_text_by_newline(text, safe_max_tokens)
    else:
        text_parts = [text]
        logger.info("El texto cabe en un solo fragmento, no es necesario dividir")

    all_tagged = []
    all_entities = {}

    for idx, part in enumerate(text_parts):
        logger.info(f"Procesando fragmento {idx + 1}/{len(text_parts)}")
        logger.debug(f"Contenido del fragmento:\n{'-' * 30}\n{part}\n{'-' * 30}")

        payload = {
            "model": MODEL_NAME,
            "prompt": PROMPT_BASE.format(texto_clinico=part),
            "stream": False,
            "options": {
                "num_ctx": max_tokens
            }
        }

        if USE_GRAMMAR:
            payload["grammar"] = get_gbnf_grammar()

        try:
            logger.debug("Enviando solicitud al modelo LLM...")
            response = requests.post(f"http://localhost:{ollama_port}/api/generate", json=payload,
                                     headers={"Content-Type": "application/json"}, timeout=1000)
            response.raise_for_status()
            result = response.json()['response']

            logger.debug(f"Respuesta del LLM para fragmento {idx + 1}:")
            logger.debug(f"{'-' * 30}\n{result}\n{'-' * 30}")

            # Intentamos parsear el JSON del modelo y acumular entidades
            try:
                parsed = ast.literal_eval(result)
                fragment_entities = parsed.get("entidades", {})

                logger.info(f"Entidades encontradas en fragmento {idx + 1}:")
                for tag, values in fragment_entities.items():
                    logger.info(f"- {tag}: {values}")
                    if tag not in all_entities:
                        all_entities[tag] = set()
                    all_entities[tag].update(values)

                all_tagged.append(parsed.get("texto_anotado", part))
            except Exception as parse_error:
                logger.error(f"Error parseando JSON en fragmento {idx + 1}: {parse_error}")
                logger.error(f"Respuesta cruda del modelo:\n{result}")
                all_tagged.append(part)

        except requests.exceptions.RequestException as e:
            logger.error(f"Error al procesar fragmento {idx + 1}: {e}")
            all_tagged.append(part)

    # Convertimos sets a listas para salida limpia
    all_entities = {k: list(v) for k, v in all_entities.items()}

    logger.info("Resumen final de entidades encontradas:")
    for tag, values in all_entities.items():
        logger.info(f"- {tag}: {values}")

    return {
        'tagged_text': "\n\n".join(all_tagged),
        'entities': all_entities
    }


def process_xml_files(input_dir, output_dir, ollama_port):
    """Procesa archivos XML manteniendo ambas salidas."""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # Obtener lista de archivos XML
    xml_files = [f for f in os.listdir(input_dir) if f.endswith('.xml')]

    if not xml_files:
        logger.warning(f"No se encontraron archivos XML en el directorio de entrada: {input_dir}")
        return

    logger.info(f"Iniciando procesamiento de {len(xml_files)} archivos XML...")

    # Barra de progreso con tqdm
    for filename in tqdm(xml_files, desc="Procesando archivos", unit="archivo"):
        try:
            input_path = os.path.join(input_dir, filename)
            output_path = os.path.join(output_dir, filename)

            logger.info(f"Procesando archivo: {filename}")

            # Leer XML original
            tree = ET.parse(input_path)
            original_text = tree.find('.//TEXT').text or ""

            logger.debug("Texto original extra√≠do del XML:")
            logger.debug(f"{'-' * 30}\n{original_text}\n{'-' * 30}")

            # Extraer datos (texto marcado + entidades)
            result = extract_entities(original_text, ollama_port)

            logger.debug("\nTexto marcado final:")
            logger.debug(f"{'-' * 30}\n{result['tagged_text']}\n{'-' * 30}")

            # Crear XML estilo MEDDOCAN
            meddocan_xml = build_meddocan_xml_from_entities(original_text, result['entities'])

            # Embellecer XML
            pretty_xml = minidom.parseString(meddocan_xml).toprettyxml(indent="  ")

            # Guardar XML en salida
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(pretty_xml)

            logger.info(f"Resultado guardado en: {output_path}")

        except Exception as e:
            logger.error(f"Error procesando {filename}: {str(e)}")


def process_single_xml_file(input_file_path, output_dir, ollama_port, model_name, prompt_text, usegrammar):
    """Procesa un √∫nico archivo XML y devuelve True si se guard√≥ correctamente, False en caso contrario.

    Args:
        input_file_path (str): Ruta completa al archivo XML de entrada
        output_dir (str): Directorio de salida donde guardar el resultado
        ollama_port (str): Puerto de Ollama a utilizar
        model_name (str): Nombre del modelo a utilizar (debe estar en MODEL_CONTEXT_LIMITS)
        prompt_text (str): Texto del prompt a utilizar para el procesamiento

    Returns:
        bool: True si el archivo se proces√≥ y guard√≥ correctamente, False si hubo alg√∫n error
    """
    try:
        # Validar que el modelo existe
        if model_name not in MODEL_CONTEXT_LIMITS:
            logger.error(f"Modelo '{model_name}' no encontrado en MODEL_CONTEXT_LIMITS")
            return False

        # Configurar el modelo global para esta ejecuci√≥n
        global MODEL_NAME
        MODEL_NAME = model_name

        # Configurar el prompt base
        global PROMPT_BASE
        PROMPT_BASE = prompt_text

        global USE_GRAMMAR
        USE_GRAMMAR = usegrammar

        # Crear directorio de salida si no existe
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        logger.info(f"Procesando archivo: {input_file_path}")
        logger.info(f"Modelo: {model_name}")
        logger.info(f"Directorio de salida: {output_dir}")

        # Leer XML original
        tree = ET.parse(input_file_path)
        original_text = tree.find('.//TEXT').text or ""

        logger.debug("Texto original extra√≠do del XML:")
        logger.debug(f"{'-' * 30}\n{original_text}\n{'-' * 30}")

        # Extraer datos (texto marcado + entidades)
        result = extract_entities(original_text, ollama_port)

        logger.debug("\nTexto marcado final:")
        logger.debug(f"{'-' * 30}\n{result['tagged_text']}\n{'-' * 30}")

        # Crear XML estilo MEDDOCAN
        meddocan_xml = build_meddocan_xml_from_entities(original_text, result['entities'])

        # Embellecer XML
        pretty_xml = minidom.parseString(meddocan_xml).toprettyxml(indent="  ")

        # Guardar XML en salida
        output_filename = os.path.basename(input_file_path)
        output_path = os.path.join(output_dir, output_filename)

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(pretty_xml)

        logger.info(f"Resultado guardado correctamente en: {output_path}")
        return True

    except ET.ParseError as e:
        logger.error(f"Error al parsear XML {input_file_path}: {str(e)}")
        return False
    except IOError as e:
        logger.error(f"Error de E/S al procesar {input_file_path}: {str(e)}")
        return False
    except Exception as e:
        logger.error(f"Error inesperado procesando {input_file_path}: {str(e)}")
        return False


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Procesador de archivos XML cl√≠nicos con Ollama')
    parser.add_argument('--port', type=str, default=DEFAULT_OLLAMA_PORT,
                        help=f'Puerto de Ollama (por defecto: {DEFAULT_OLLAMA_PORT})')

    args = parser.parse_args()

    logger.info("INICIANDO PROCESAMIENTO DE ARCHIVOS XML")
    logger.info(f"Usando puerto Ollama: {args.port}")

    process_xml_files('test/xml', 'output/xml/LLaMA_model3.3', args.port)

    logger.info("PROCESAMIENTO COMPLETADO")